---
---

@inproceedings{NEURIPS2024_f9c2ab8d,
 author = {S\'{u}ken\'{\i}k, Peter and Lampert, Christoph and Mondelli, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {138250--138288},
 publisher = {Curran Associates, Inc.},
 title = {Neural collapse vs. low-rank bias: Is deep neural collapse really optimal?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f9c2ab8d429044e0c35bcece2ff6d123-Paper-Conference.pdf},
 volume = {37},
 selected={true},
 year = {2024}
}

@article{beaglehole2024average,
  title={Average gradient outer product as a mechanism for deep neural collapse},
  author={Beaglehole, Daniel and S{\'u}ken{\'\i}k, Peter and Mondelli, Marco and Belkin, Misha},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={130764--130796},
  year={2024}
}

@article{sukenik2023deep,
  title={Deep neural collapse is provably optimal for the deep unconstrained features model},
  author={S{\'u}ken{\'\i}k, Peter and Mondelli, Marco and Lampert, Christoph H},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={52991--53024},
  selected={true},
  year={2023}
}

@InProceedings{pmlr-v162-sukeni-k22a,
  title = 	 {Intriguing Properties of Input-Dependent Randomized Smoothing},
  author =       {S{\'u}ken\'{\i}k, Peter and Kuvshinov, Aleksei and G{\"u}nnemann, Stephan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {20697--20743},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/sukeni-k22a/sukeni-k22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/sukeni-k22a.html},
  abstract = 	 {Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as “certified accuracy waterfalls”, certification vs. accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance function and test it on CIFAR10 and MNIST. Our design mitigates some of the problems of classical smoothing and is formally underlined, yet further improvement of the design is still necessary.}
}

@article{jacot2024wide,
  title={Wide neural networks trained with weight decay provably exhibit neural collapse},
  author={Jacot, Arthur and S{\'u}ken{\'\i}k, Peter and Wang, Zihan and Mondelli, Marco},
  journal={ICLR},
  year={2025}
}

@article{kocsis2022unreasonable,
  title={The unreasonable effectiveness of fully-connected layers for low-data regimes},
  author={Kocsis, Peter and S{\'u}ken{\'\i}k, Peter and Bras{\'o}, Guillem and Nie{\ss}ner, Matthias and Leal-Taix{\'e}, Laura and Elezi, Ismail},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1896--1908},
  year={2022}
}

@article{sukenik2024generalization,
  title={Generalization in multi-objective machine learning},
  author={S{\'u}ken{\'\i}k, Peter and Lampert, Christoph},
  journal={Neural Computing and Applications},
  pages={1--15},
  year={2024},
  publisher={Springer}
}

@article{sukenik2025neural,
  title={Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers},
  author={S{\'u}ken{\'\i}k, Peter and Lampert, Christoph H and Mondelli, Marco},
  journal={arXiv preprint arXiv:2505.15239},
  selected={true},
  year={2025}
}